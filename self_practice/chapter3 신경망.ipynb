{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter3  신경망\n",
    "- 퍼셉트론과 신경망이 차이점?\n",
    "- 시그모이드 함수,ReLU\n",
    "- 다차원 배열 계산\n",
    "- 소프트맥수 함수\n",
    "\n",
    "#### 출처 : 밑바닥 부터 시작하는 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.1 퍼셉트론에서 신경망으로 \n",
    "### 3.1.1 신경망의 예\n",
    "- 입력층, 출력층, 은닉층으로 이루어짐\n",
    "- x1과 x2를 인수로 받는  AND 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 활성화 함수의 등장\n",
    "입력 신호의 총합을 출력 신호로 변환하는 함수 : 활성화 함수 (h(x))\n",
    "단일 퍼셉트론의 경우는 활성화함수로 계단함수를 사용 \n",
    "신경망의 경운은 시그모이드 함수 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#계단 함수 구현하기1 (실수만 받음)\n",
    "def step_function1(x):\n",
    "    if x >0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#계단함수 구현하기2 ( 넘파이 배열 받음)\n",
    "def step_function2(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.int)\n",
    "\n",
    "## 앞으로의 편의성을 위해서 step_function2 사용 \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x >0, dtype= np.int)\n",
    "\n",
    "x = np.arange(-5.0,5.0,0.1)\n",
    "y = step_function2(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#시그모이드 함수 구현하기\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0,5.0,0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 시그모이드 함수와 계단 함수 비교\n",
    "퍼셉트론은 계단함수, 신경망은 시그모이드\n",
    "계단함수와 시그모이드 함수는 출력이 0~1사이라는 점에서 공통점이 있으며 둘다 비선형 함수(직선1개로 그릴수없음)라는 것도 공통점이다.\n",
    "\n",
    "### 3.2.6 비선형 함수\n",
    "신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. \n",
    "은닉층 때문이다. 은닉층이 없으면 신경망이라고 할수 없다.\n",
    "앞서 XOR를 구현했던 예를 생각하면 쉬울것이다.\n",
    "선형함수는 층을 아무리 깊게해도 은닉층 없는 네트워크로도 똑같은 기능을 할수 있다. \n",
    "선형함수를 이용해서는 여러 층으로 구성하는 이점을 살릴수 없다. \n",
    "층을 쌓는 혜택을 얻고 싶다면 비선형 함수를 사용해야한다\n",
    "\n",
    "### 3.2.7 ReLU함수\n",
    "계단함수, 시그모이드 함수 이외 최근에 많이 쓰는 활성화 함수.\n",
    "입력이 0이 넘으면 입력을 그대로 출력, 0이하면 0을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU 구현\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 신경망의 내적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "(2, 3)\n",
      "[ 5 11 17]\n"
     ]
    }
   ],
   "source": [
    "#넘파이 행렬을 써서 신경망 구현하기\n",
    "#X(입력), W(가중치), Y(출력)의 형상 주의. 특히 X와 W의 대응하는 차원의 원소 수가 같아야 함(행렬곱이 가능하도록!)\n",
    "\n",
    "X = np.array([1,2])\n",
    "print(X.shape)\n",
    "W = np.array([[1,3,5],[2,4,6]])\n",
    "print(W)\n",
    "print(W.shape)\n",
    "Y = np.dot(X,W)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 3층 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n",
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "#입력층에서 1층으로 신호 전달\n",
    "\n",
    "X = np.array([1.0,0.5]) #입력은 두개\n",
    "W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])  #두개의 입력에 대한 1층의 가중치\n",
    "B1 = np.array([0.1,0.2,0.3])# 전달된 입력에 대한 1층의 편향\n",
    "\n",
    "print(W1.shape)#(2,3)\n",
    "print(X.shape)#(2,)\n",
    "print(B1.shape)#(3,)\n",
    "\n",
    "A1 = np.dot(X,W1) + B1\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(A1)\n",
    "print(Z1)\n",
    "\n",
    "#1층에서 2층으로 신호전달\n",
    "\n",
    "W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "B2 = np.array([0.1,0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "#2층에서 출력층으로 신호 전달\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1,0.3],[0.2,0.4]])\n",
    "B3 = np.array([0.1,0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 구현정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "#위의 과정을 좀 그럴듯 하게 다시 간단히 구현하면\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "    network['b1'] = np.array([0.1,0.2,0.3])\n",
    "    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "    network['b2'] = np.array([0.1,0.2])\n",
    "    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])\n",
    "    network['b3'] = np.array([0.1,0.2])\n",
    "    \n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x,W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1,W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2,W3) + b3\n",
    "    y = identity_function(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0,0.5])\n",
    "y = forward(network,x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 출력층 설계하기 \n",
    "\n",
    "- 신경망은 분류와 회귀 모두에 이용할 수 있다.\n",
    "- 다만 둘중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라짐\n",
    " 회귀 : 항등함수 ( 입력 그대로 출력이됨)\n",
    " 분류 : 소프트맥스 함수 (입력신호의 지수 함수 / 모든 입력 신호의 지수 합수의 합) (출력이 0에서 1.0사이의 실수 값이며, 출력의 총합이 1)\n",
    "        => 이런 성질로 인해서 소프트맥수 합수의 출력을 '확률'로 해석이 가능함\n",
    "        \n",
    "- 출력층 뉴런 수 : 풀려는 문제에 맞게 적절히 정하기. 보통은 분류에서 분류하고 싶은 클래스 수로 설정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 손글씨 숫자 인식\n",
    "- 이미 학습된 매개변수 사용\n",
    "- 학습과정 생략하고 추론과정만 구현 \n",
    "- 신경망의 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) #부모 디렉터리 파일을 가져올 수 있도록 설정\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#처음 한 번은 몇 분 정도 걸림\n",
    "(x_train, t_train), (x_test, t_test) =  load_mnist(flatten=True,normalize=False)\n",
    "\n",
    "#각 데이터의 형상 출력\n",
    "print(x_train.shape) #(60000,784)\n",
    "print(t_train.shape) #(60000,)\n",
    "print(x_test.shape) #(10000, 784)\n",
    "print(t_test.shape) #(10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "    \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True,normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label) #5\n",
    "\n",
    "print(img.shape) #(784,)\n",
    "img = img.reshape(28, 28) # 원래 이미지의 모양으로 변형\n",
    "print(img.shape) #(28,28)\n",
    "\n",
    "img_show(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 신경망의 추론 처리\n",
    "-MNIST 데이터셋을 가지고 추론 수행하는 신경망 구현하기\n",
    "- 입력층 뉴런 : 784개, 이미지의 크기가 28x28 (= 784)\n",
    "- 출력층 뉴런 : 10개 :분류문제로 0 에서 9 까지 구분하는 문제\n",
    "- 은닉층 총 2개 : 각 50 ,100개 : 임의로 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-89e54a01d137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# MNIST 데이터셋 가져오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 네트워크 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-89e54a01d137>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_laber\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 전처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_mnist' is not defined"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    (x_train, t_train),(x_test,t_test) = \\\n",
    "    load_mnist(normalize=True,flatten=True, one_hot_laber=False) # 전처리\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\",'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "        \n",
    "        return network\n",
    "    \n",
    "def predict(network, x): #각 레이블의 확률을 넘파이 배열로 반환\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x,W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1,W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2,W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "x, t = get_data() # MNIST 데이터셋 가져오기\n",
    "network = init_network() # 네트워크 생성\n",
    "\n",
    "accurary_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i]) #x에 저장된 이미지 데이터를 1장씩 꺼내서 predict()함수로 분류\n",
    "    p = np.argmax(y) #확률이 가장 높은 원소의 인덱스를 얻는다 (=흐름상 가장 확률의 높은 글자를 찾게됨) = 예측결과\n",
    "    if p == t[i]: #예측결과와 정답레이블 비교하여 정확도를 구함\n",
    "        accuracy_cnt += 1 \n",
    "        \n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt)/len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 신경망 학습\n",
    "\n",
    "-학습 : 훈련데이터로 부터 가중치 매개변수의 최적 값을 자동으로 획득\n",
    "-손실함수 : 신경망이 학습할 수 있도록 해주는 지표(손실함수의 결과값ㅇ르 가장 작게 만드는 가중치 매개변수를 찾는것이 학습의 목표)\n",
    "\n",
    "## 4.1 데이터에서 학습한다!\n",
    "신경망은 데이터를 보고 학습할 수 있다.(가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다)\n",
    "\n",
    "### 4.1.1 데이터 주도학습\n",
    "지도학습 : 사람이 신경씀 + 머신러닝\n",
    "비지도학습 : 사람이 신경안씀 : 딥러닝\n",
    "\n",
    "### 4.1.2 훈련 데이터와 시험 데이터\n",
    "\n",
    " >데이터 셋을 두개로 나눈다.\n",
    "   -훈련 데이터 : 학습하면서 최적의 매개변수를 찾음\n",
    "   -시험 데이터 : 앞서 트레이닝된 모델을 평가\n",
    "   : 이렇게 두개의 군집으로 나누는 이유는 범용 능력을 평가하고 싶어서\n",
    "   오버피팅 : 특정 데이터셋에만 지나치게 최적화되서 범용 능력이 떨어짐\n",
    "   \n",
    "## 4.2 손실 함수\n",
    "손실함수 : 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현하고 그 지표를 가장 좋게 만들어주는 가중치 매개변수를의 값을 탐색한다. \n",
    "           신경망 학습에서 사용하는 지표는 '손실함수'이며 손실함수로 보통 평균제곰오차와 교차 엔트로피 오차를 사용한다. \n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n",
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "### 4.2.1 평균 제곱 오차\n",
    "#가장 많이 쓰는 손실함수\n",
    "\n",
    "\n",
    "#     0    1   2   3   4    5   6   7   8   9\n",
    "y1 = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]#소프트맥스 함수 출력 결과로, 확률로 설명가능, 2의 확률이 0.6으로 제일 높음\n",
    "y2 = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]#여기서는 7일확률이 0.6으로 제일 높음\n",
    "\n",
    "t = [0,0,1,0,0,0,0,0,0,0]#숫자 2에 해당하는 원소 값이 1이미 정답이 '2' => 정답만 1로 표기 나머지는 0으로 나타냄 = 원-핫 인코딩\n",
    "\n",
    "\n",
    "#평균제곱오차구현\n",
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "\n",
    "print(mean_squared_error(np.array(y1),np.array(t)))\n",
    "#0.09750000000000003  :y1이 정답이 확률 약 97%\n",
    "\n",
    "print(mean_squared_error(np.array(y2),np.array(t)))\n",
    "#0.5975  :y2가 정답일 확률 약 59%\n",
    "\n",
    "#평균 제곰 오차를 기준으로 첫번째 추정결과가 더 정답에 가까움\n",
    "\n",
    "### 4.2.2 교차 엔트로피 오차\n",
    "#정답일때의 출력이 전체 값을 정한다.\n",
    "#예시에서 정답은 \"2\" ,이때의 신경망 출력은 0.6 => 교차 엔트로피오차는 0.51 같은 조건에서 신경망 출력이 0.1(오답)이라면 2.30으로 오차가 커짐. \n",
    "#교차엔트로피오차 구현\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "\n",
    "print(cross_entropy_error(np.array(y1),np.array(t)))\n",
    "#0.510825457099338 \n",
    "\n",
    "print(cross_entropy_error(np.array(y2),np.array(t)))\n",
    "#2.302584092994546\n",
    "#교차엔트로피오차를 기준으로 첫번째 추정결과가 더 정답에 가까움\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "훈련 데이터에 대한 손실 함수 값을 구하고(오차값은 최소가 되어야함), 그 값을 최대한 줄여주는 매개변수를 찾아냄\n",
    "훈련데이터가 100개가 있으면 그로부터 계산한 100개의 오차값의 평균을 구함.\n",
    "훈련데이터가 너무너무 많을 경우 임의로 개수만큼 데이터를 무작위로 골라내서 학습하는데 이를 미니 배치학습이라고함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "[ 8867 20904  6379   391  9729 17997 56447 39453 20874 33596]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "## normalize :정규화된 데이터만 배열을 얻음, one_hot_label:정답 위치의 원소만 1이고 나머지가 0인 배열을 얻음\n",
    "(x_train, t_train),(x_test,t_test) = load_mnist(normalize= True, one_hot_label=True)\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000, 10)\n",
    "\n",
    "train_size = x_train.shape[0] #60000\n",
    "batch_size = 10 #10\n",
    "# 60000미만의 수에서 무작위로 10개를 고름\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask) #무작위로 뽑힌 10개의 인덱스를 확인할수 있다\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "        \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size),t])) / batch_size\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 손실함수를 설정하는 이유\n",
    "\n",
    "학습의 목표는 '정확도'를 끌어내는 매개변수값을 찾는 것이다. \n",
    "그런데 왜 '정확도'라는 지표를 놔두고 '손실함수의 값'이라는 우회적인 방법을 선택하는 걸까?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 미분\n",
    "- 미분 = 한 순간의 변화량\n",
    "- 수치 미분 : 오차를 포함하는 미분 => 딥러닝에서 말하는 미분\n",
    "- 해석적미분 : 고등학교때 배우는 미분\n",
    "- 편미분 : 변수가 여럿인 함수에 대한 미분\n",
    "신경망에서는 편미분을 사용하는데 모든 가중치 변수에대한 기울기를 구해야하기 때문\n",
    "\n",
    "## 4.4 기울기\n",
    "모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 라고 한다.\n",
    "기울기는 함수의 '가장 낮은 장소(최솟값)'가리킨다. \n",
    "기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다.\n",
    "\n",
    "### 4.4.1 경사법(경사 하강법)\n",
    "머신러닝 -> 러닝 단계에서 최적의 매개변수를 찾음.\n",
    "신경망역시 최적(=손실 함수가 최소가 되는 값) 배개변수(가중치와 편향)을 러닝을 통해 찾아야함. \n",
    "기울기를 잘 이용해 함수의 최솟값( 가능한한 작은값)을 찾는것이 것이 경사법 \n",
    "경사 하강 법을 통해서 머신러닝 최적화를 한다.\n",
    "학습률 : 한번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값이 얼마나 갱신하는냐를 정하느 것, 너무 작아도 너무 커도 문제.\n",
    "\n",
    "### 4.4.2 신경망에서의 기울기\n",
    "신경망 학습에서의 기울기 = 가중지 매개변수에 대한 손실 함수의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57822294 -1.05095302 -0.56104759]\n",
      " [ 0.16814516  0.33520498  0.21379606]]\n",
      "[-0.19560312 -0.32888733 -0.1442121 ]\n",
      "[[ 0.20492142  0.17935057 -0.38427199]\n",
      " [ 0.30738213  0.26902586 -0.57640799]]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #정규분포로 초기화 \n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "net = simpleNet()\n",
    "print(net.W) #가중치 매개변수\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)\n",
    "\n",
    "t = np.array([0, 0, 1]) #정답 레이블\n",
    "net.loss(x,t)\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dW = numerical_gradient(f,net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "신경망 학습의 절차\n",
    "\n",
    "전제\n",
    " 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라한다.\n",
    " \n",
    " 1단계 - 미니 배치 : 훈련데이터중 일부를 무작위로 가져옴\n",
    " 2단계 - 기울기 산출 : 미니배치의 손실함수 값을 줄이기 위해 각가중치의 매개변수 기울기를 구한다.\n",
    " 3단계 = 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    " 4단계 = 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5.2 2층 신경망 클래스 구현하기 \n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis =1)\n",
    "        t = np.argmax(t, axis =1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    #x :입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "    net.params['W1'].shape #(784,100)\n",
    "    net.params['b1'].shape #(100,)\n",
    "    net.params['W2'].shape #(100,10)\n",
    "    net.params['b2'].shape #(10,)\n",
    "    \n",
    "    x = np.random.rand(100,784) #더미 입력 데이터(100장 분량)\n",
    "    y = net.predict(x)\n",
    "    \n",
    "    t = np.random.rand(100,10) #더미 정답 레이블(100장 분량)\n",
    "    \n",
    "    grads = net.numerical_gradient(x,t) #기울기 계산\n",
    "    grads['W1'].shape #(784, 100)\n",
    "    grads['b1'].shape #(100,)\n",
    "    grads['W2'].shape #(100, 10)\n",
    "    grads['b2'].shape #(10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5.2 미니배치 학습 구현하기 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "#하이퍼파라미터\n",
    "iters_num = 10000 #반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 #미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch) #성능 개선판!\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10218333333333333, 0.101\n",
      "train acc, test acc | 0.782, 0.7903\n",
      "train acc, test acc | 0.8777833333333334, 0.8814\n",
      "train acc, test acc | 0.8986833333333333, 0.9015\n",
      "train acc, test acc | 0.9077, 0.9109\n",
      "train acc, test acc | 0.9147166666666666, 0.9157\n",
      "train acc, test acc | 0.9189, 0.9204\n",
      "train acc, test acc | 0.9244, 0.926\n",
      "train acc, test acc | 0.9271333333333334, 0.9286\n",
      "train acc, test acc | 0.9311166666666667, 0.9315\n",
      "train acc, test acc | 0.93435, 0.9336\n",
      "train acc, test acc | 0.93685, 0.9363\n",
      "train acc, test acc | 0.94, 0.9383\n",
      "train acc, test acc | 0.9424, 0.9424\n",
      "train acc, test acc | 0.94385, 0.9431\n",
      "train acc, test acc | 0.94545, 0.9442\n",
      "train acc, test acc | 0.9478166666666666, 0.945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "#하이퍼파라미터\n",
    "iters_num = 10000  #반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 #미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "#미니배치 획득\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    #학습경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 수고하셨습니다"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
